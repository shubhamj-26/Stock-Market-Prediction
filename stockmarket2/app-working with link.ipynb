{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#importing required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "#to plot within notebook\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from yahoo_fin import stock_info as si\n",
    "\n",
    "def getpriceinfo(symbol):\n",
    "    lp=si.get_live_price(symbol)\n",
    "\n",
    "    \n",
    " \n",
    "    #print(si.get_day_most_active())\n",
    " \n",
    "    # get biggest gainers\n",
    "    #print(si.get_day_gainers())\n",
    " \n",
    "    # get worst performers\n",
    "    #print(si.get_day_losers())\n",
    "    return lp\n",
    "\n",
    "def getqoutetable(symbol):\n",
    "    \n",
    "\n",
    "    qt=si.get_quote_table(symbol, dict_result = False)\n",
    " \n",
    "    #print(si.get_day_most_active())\n",
    " \n",
    "    # get biggest gainers\n",
    "    #print(si.get_day_gainers())\n",
    " \n",
    "    # get worst performers\n",
    "    #print(si.get_day_losers())\n",
    "    return qt\n",
    "\n",
    "# function to calculate percentage difference considering baseValue as 100%\n",
    "def percentageChange(baseValue, currentValue):\n",
    "    return((float(currentValue)-baseValue) / abs(baseValue)) *100.00\n",
    "\n",
    "# function to get the actual value using baseValue and percentage\n",
    "def reversePercentageChange(baseValue, percentage):\n",
    "    return float(baseValue) + float(baseValue * percentage / 100.00)\n",
    "\n",
    "# function to transform a list of values into the list of percentages. For calculating percentages for each element in the list\n",
    "# the base is always the previous element in the list.\n",
    "def transformToPercentageChange(x):\n",
    "    baseValue = x[0]\n",
    "    x[0] = 0\n",
    "    for i in range(1,len(x)):\n",
    "        pChange = percentageChange(baseValue,x[i])\n",
    "        baseValue = x[i]\n",
    "        x[i] = pChange\n",
    "\n",
    "# function to transform a list of percentages to the list of actual values. For calculating actual values for each element in the list\n",
    "# the base is always the previous calculated element in the list.\n",
    "def reverseTransformToPercentageChange(baseValue, x):\n",
    "    x_transform = []\n",
    "    for i in range(0,len(x)):\n",
    "        value = reversePercentageChange(baseValue,x[i])\n",
    "        baseValue = value\n",
    "        x_transform.append(value)\n",
    "    return x_transform\n",
    "\n",
    "#read the data file\n",
    "def predictpriceofdata(stockname):\n",
    "    df = pd.read_csv('data\\\\'+stockname+'.csv')\n",
    "# store the first element in the series as the base value for future use.\n",
    "    baseValue = df['Close'][0]\n",
    "\n",
    "# create a new dataframe which is then transformed into relative percentages\n",
    "    data = df.sort_index(ascending=True, axis=0)\n",
    "    new_data = pd.DataFrame(index=range(0,len(df)),columns=['Date', 'Close'])\n",
    "    for i in range(0,len(data)):\n",
    "        new_data['Date'][i] = data['Date'][i]\n",
    "        new_data['Close'][i] = data['Close'][i]\n",
    "\n",
    "# transform the 'Close' series into relative percentages\n",
    "    transformToPercentageChange(new_data['Close'])\n",
    "\n",
    "# set Dat column as the index\n",
    "    new_data.index = new_data.Date\n",
    "    new_data.drop('Date', axis=1, inplace=True)\n",
    "\n",
    "# create train and test sets\n",
    "    dataset = new_data.values\n",
    "    train, valid = train_test_split(dataset, train_size=0.99, test_size=0.01, shuffle=False)\n",
    "\n",
    "# convert dataset into x_train and y_train.\n",
    "# prediction_window_size is the size of days windows which will be considered for predicting a future value.\n",
    "    prediction_window_size = 60\n",
    "    x_train, y_train = [], []\n",
    "    for i in range(prediction_window_size,len(train)):\n",
    "        x_train.append(dataset[i-prediction_window_size:i,0])\n",
    "        y_train.append(dataset[i,0])\n",
    "    x_train, y_train = np.array(x_train), np.array(y_train)\n",
    "    x_train = np.reshape(x_train, (x_train.shape[0],x_train.shape[1],1))\n",
    "\n",
    "##################################################################################################\n",
    "# create and fit the LSTM network\n",
    "# Initialising the RNN\n",
    "    model = Sequential()\n",
    "# Adding the first LSTM layer and some Dropout regularisation\n",
    "    model.add(LSTM(units = 50, return_sequences = True, input_shape = (x_train.shape[1], 1)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "# Adding a second LSTM layer and some Dropout regularisation\n",
    "    model.add(LSTM(units = 50, return_sequences = True))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "# Adding a third LSTM layer and some Dropout regularisation\n",
    "    model.add(LSTM(units = 50, return_sequences = True))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "# Adding a fourth LSTM layer and some Dropout regularisation\n",
    "    model.add(LSTM(units = 50))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "# Adding the output layer\n",
    "    model.add(Dense(units = 1))\n",
    "# Compiling the RNN\n",
    "    model.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "\n",
    "# Fitting the RNN to the Training set\n",
    "    model.fit(x_train, y_train, epochs = 1, batch_size = 1000)\n",
    "\n",
    "##################################################################################################\n",
    "\n",
    "#predicting future values, using past 60 from the train data\n",
    "# for next 10 yrs total_prediction_days is set to 3650 days\n",
    "    total_prediction_days = 3650\n",
    "    inputs = new_data[-total_prediction_days:].values\n",
    "    inputs = inputs.reshape(-1,1)\n",
    "\n",
    "# create future predict list which is a two dimensional list of values.\n",
    "# the first dimension is the total number of future days\n",
    "# the second dimension is the list of values of prediction_window_size size\n",
    "    X_predict = []\n",
    "    for i in range(prediction_window_size,inputs.shape[0]):\n",
    "        X_predict.append(inputs[i-prediction_window_size:i,0])\n",
    "    X_predict = np.array(X_predict)\n",
    "\n",
    "# predict the future\n",
    "    X_predict = np.reshape(X_predict, (X_predict.shape[0],X_predict.shape[1],1))\n",
    "    future_closing_price = model.predict(X_predict)\n",
    "\n",
    "    train, valid = train_test_split(new_data, train_size=0.99, test_size=0.01, shuffle=False)\n",
    "    date_index = pd.to_datetime(train.index)\n",
    "\n",
    "#converting dates into number of days as dates cannot be passed directly to any regression model\n",
    "    x_days = (date_index - pd.to_datetime('1970-01-01')).days\n",
    "\n",
    "# we are doing prediction for next 5 years hence prediction_for_days is set to 1500 days.\n",
    "    prediction_for_days = 300\n",
    "    future_closing_price = future_closing_price[:prediction_for_days]\n",
    "\n",
    "# create a data index for future dates\n",
    "    x_predict_future_dates = np.asarray(pd.RangeIndex(start=x_days[-1] + 1, stop=x_days[-1] + 1 + (len(future_closing_price))))\n",
    "    future_date_index = pd.to_datetime(x_predict_future_dates, origin='1970-01-01', unit='D')\n",
    "\n",
    "# transform a list of relative percentages to the actual values\n",
    "    train_transform = reverseTransformToPercentageChange(baseValue, train['Close'])\n",
    "\n",
    "# for future dates the base value the the value of last element from the training set.\n",
    "    baseValue = train_transform[-1]\n",
    "    valid_transform = reverseTransformToPercentageChange(baseValue, valid['Close'])\n",
    "    future_closing_price_transform = reverseTransformToPercentageChange(baseValue, future_closing_price)\n",
    "\n",
    "# recession peak date is the date on which the index is at the bottom most position.\n",
    "    recessionPeakDate =  future_date_index[future_closing_price_transform.index(min(future_closing_price_transform))]\n",
    "    minCloseInFuture = min(future_closing_price_transform);\n",
    "    print(\"The stock market will reach to its lowest bottom on\", recessionPeakDate)\n",
    "    print(\"The lowest index the stock market will fall to is \", minCloseInFuture)\n",
    "\n",
    "# plot the graphs\n",
    "    plt.figure(figsize=(16,8))\n",
    "    df_x = pd.to_datetime(new_data.index)\n",
    "    plt.plot(date_index,train_transform, label='Close Price History')\n",
    "    plt.plot(future_date_index,future_closing_price_transform, label='Predicted Close')\n",
    "\n",
    "# set the title of the graph\n",
    "    plt.suptitle('Stock Market Predictions', fontsize=16)\n",
    "\n",
    "# set the title of the graph window\n",
    "    fig = plt.gcf()\n",
    "    fig.canvas.set_window_title('Stock Market Predictions')\n",
    "\n",
    "#display the legends\n",
    "    plt.legend()\n",
    "\n",
    "#display the graph\n",
    "    plt.show()\n",
    "    \n",
    "    dictofdateandprice={}\n",
    "    \n",
    "    for i in range(38,50):\n",
    "        dictofdateandprice[str(future_date_index[i])]=future_closing_price_transform[i]\n",
    "    return dictofdateandprice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n    try:\\n        price = str(content).split(\\'data-reactid=\"32\"\\')[4].split(\\'</span>\\')[0].replace(\\'>\\',\\'\\')\\n        #print(str(content).split(\\'data-reactid=\"47\"\\'))\\n        openprice = str(content).split(\\'data-reactid=\"49\"\\')[3].split(\\'</span>\\')[0].replace(\\'>\\',\\'\\')\\n        rangeobt = str(content).split(\\'data-reactid=\"67\"\\')[2].split(\\'</span>\\')[0]\\n        #price = str(content).split(\\'data-reactid=\"32\"\\')[4].split(\\'</span>\\')[0].replace(\\'>\\',\\'\\')\\n        #price = str(content).split(\\'data-reactid=\"32\"\\')[4].split(\\'</span>\\')[0].replace(\\'>\\',\\'\\')\\n        #price = str(content).split(\\'data-reactid=\"32\"\\')[4].split(\\'</span>\\')[0].replace(\\'>\\',\\'\\')\\n    except IndexError as e:\\n        price = 0.00\\n        price = price or \"0\"\\n    try:\\n        price = float(price.replace(\\',\\',\\'\\'))\\n    except ValueError as e:\\n        price = 0.00\\n        time.sleep(1)\\n   \\n    print( price)\\n    print(openprice)\\n    print(rangeobt)\\n        #cursor.execute(_SQL, (unidecode.unidecode(ticker[0]), price, unidecode.unidecode(ticker[1]), unidecode.unidecode(ticker[2]), unidecode.unidecode(ticker[3])))\\n    return price\\n\\n\\n\\n#urltofetch=\\'https://www.usatoday.com/story/money/2020/04/22/amazon-doing-free-deliveries-food-banks-during-coronavirus-emergency/2997254001/\\'\\n\\n#alldata=parsenews(urltofetch)\\n#print(alldata)\\n\\n#Python program to scrape website  \\n#and save quotes from website \\nimport requests \\nfrom bs4 import BeautifulSoup \\nimport csv \\nimport re\\nimport pandas as pd\\nfrom hdfs import InsecureClient\\nimport os\\n\\nclient_hdfs = InsecureClient(\\'http://localhost:50070\\')\\n\\ndef callingnews(query):\\n\\n    URL = \"https://www.usatoday.com/search/?q=\"+query\\n    r = requests.get(URL) \\n#print(r)\\n  \\n    soup1 = BeautifulSoup(r.content, \\'html.parser\\') \\n#print(soup)\\n    \\n    df = pd.DataFrame(data = soup1)\\n    #Writing Dataframe to hdfs\\n    with client_hdfs.write(\\'/in/quotes.txt\\', encoding = \\'utf-8\\') as writer:\\n        df.to_csv(writer)\\n        \\n    with client_hdfs.read(\\'/in/quotes.txt\\', encoding = \\'utf-8\\') as reader:\\n        print(\\'ok\\',reader)\\n        soup = pd.read_csv(reader,index_col=0)\\n    print(soup)\\n\\n    quotes=[]  # a list to store quotes \\n  \\n\\n    table1 = soup.find_all(\\'a\\', attrs = {\\'class\\':\\'gnt_se_a gnt_se_a__hd gnt_se_a__hi\\'}) \\n    print(table1)\\n\\n#table13 = table11.get_text()\\n#print(table13) \\n\\n    table11 = soup.find_all(\\'div\\', attrs = {\\'class\\':\\'gnt_pr\\'}) \\n    print(table11)\\n    datalist=[]\\n    linksdata=[]\\n#print(table11)\\n    for ik in table11:\\n        datalist.append(ik.get_text())\\n        print(ik.get_text())\\n\\n    pos=0\\n    listtocheck=[]\\n    for ik in table1:\\n        links = re.findall(\"href=[\"\\'](.*?)[\"\\']\", str(ik))\\n        linksdata.append(\\'https://www.usatoday.com\\'+links[0])\\n        if \\'story\\' not in links[0]:\\n            listtocheck.append(pos)\\n        pos+=1\\n        print(links)\\n\\n    print(\"list check is \",listtocheck)\\n\\n    for ij in range(len(listtocheck)):\\n        print(ij)\\n        #datalist.pop(ij)\\n        #linksdata.pop(ij)\\n    #print(listtocheck[ij])\\n\\n    print(len(datalist))\\n    print(len(linksdata))\\n    return datalist,linksdata\\n\\n\\n#df\\ndf1=pd1.read_csv(\\'fortune500.csv\\')\\ndf=pd.DataFrame()\\napp = Flask(__name__)\\n\\n\\n@app.route(\"/parsenews\")\\ndef parsenews(): \\n    newsinfo = request.args.get(\\'msg\\')\\n    URL =newsinfo.rstrip().lstrip().strip()# \"https://www.hindustantimes.com/delhi-news/protest-at-delhi-s-jama-masjid-against-citizenship-act-4-metro-stations-closed-in-area/story-q7vKj5IUdIKMExw5eGBfxI.html\"\\n    #URL =\"https://www.hindustantimes.com/delhi-news/protest-at-delhi-s-jama-masjid-against-citizenship-act-4-metro-stations-closed-in-area/story-q7vKj5IUdIKMExw5eGBfxI.html\"\\n    #print repr(URL)\\n    r = requests.get(URL) \\n    #print(r)\\n    soup = BeautifulSoup(r.content, \\'html.parser\\') \\n  \\n    quotes=[]  # a list to store quotes \\n  \\n    table = soup.find(\\'div\\', attrs = {\\'class\\':\\'gnt_ar_b\\'}) \\n    #print(table)\\n    alltestdata=\\'<a href=\\'\\'+URL+\\'\\' target=\"_blank\" >\\'+URL+\\'</a>\\'+\\'<br>\\'\\n    print(alltestdata)\\n    try:\\n        table1 = table.find_all(\\'p\\')\\n        \\n        for row in table.find_all(\\'p\\'):\\n            quote = {} \\n            quote[\\'data\\'] = row.text \\n            alltestdata=alltestdata+row.text+\" \"\\n            quotes.append(quote)\\n    except:\\n        alltestdata=\\'<a href=\\'\\'+URL+\\'\\' target=\"_blank\" >\\'+URL+\\'</a>\\'+\\'<br>\\'\\n    #print(alltestdata)\\n    print(alltestdata)\\n    return alltestdata\\n\\n@app.route(\"/searchforcompany\")\\ndef searchforcompany():\\n    global df\\n    legend = \\'Stock Price data\\'\\n    company = request.args.get(\\'company\\')\\n    dfop=df1.loc[df1[\\'Name\\'] == company]\\n    op1=str(dfop[\\'Symbol\\'].iloc[0])\\n    print(op1)\\n    df=pd1.read_csv(\\'data//\\'+op1+\\'.csv\\')\\n    temperatures = list(df[\\'Close\\'])\\n    times = list(df[\\'Date\\'])\\n    \\n    datalist,linksdata=callingnews(company)\\n    dictis={}\\n    for ims in range(len(linksdata)):\\n        dictis[linksdata[ims]]=linksdata[ims]\\n        \\n    print(\"dictionary is \",dictis)\\n    urlofsite=\\'https://www.usatoday.com\\'\\n    io=0\\n    return render_template(\\'line_chart.html\\',dictdata=dictis,links=linksdata,news=datalist, values=temperatures, labels=times, legend=legend,stockname=company,symbolis=op1)\\n    #return op1\\n\\n@app.route(\"/futurepriceprediction\")\\ndef futurepriceprediction():\\n    companySymbol = request.args.get(\\'msg\\')\\n    dictis=predictpriceofdata(companySymbol)\\n    #print(\\'price is\\')\\n    print(dictis)\\n    #print(sendingcompaniesinfo)\\n    return dictis\\n\\n    \\n    \\n@app.route(\"/fetchprice\")\\ndef fetchprice():\\n    company = request.args.get(\\'msg\\')\\n    priceis=getpriceinfo(company)#\\'1211\\'#fetchcurrentmarketprice(company)\\n    print(\\'price is\\')\\n    print(priceis)\\n    #print(sendingcompaniesinfo)\\n    return str(priceis)\\n\\n\\n@app.route(\"/getqoutetableval\")\\ndef getqoutetableval():\\n    company = request.args.get(\\'msg\\')\\n    print(\\'company for qoute \\'+company)\\n    qoute=getqoutetable(company)#\\'1211\\'#fetchcurrentmarketprice(company)\\n    print(\\'qoute is\\')\\n    print(qoute)\\n    df_list = qoute.values.tolist()\\n    alldata=\\'\\'\\n    for ik in range(len(df_list)):\\n        alldata=alldata+str(df_list[ik][0])+\" :- \"+str(df_list[ik][1])+\"<br>\\n\"\\n    #JSONP_data = jsonpify(df_list)\\n    #print(sendingcompaniesinfo)\\n    return alldata\\n\\n@app.route(\"/\")\\ndef index():\\n    temperatures = dict(df1[\\'Name\\'])\\n    sendingcompaniesinfo={}\\n    for keys in temperatures: \\n        temperatures[keys] = str(temperatures[keys]) \\n        sendingcompaniesinfo[temperatures[keys]]=\\'null\\'\\n    #print(sendingcompaniesinfo)\\n    return render_template(\\'searching.html\\', values=sendingcompaniesinfo)\\n\\n\\n\\n\\n@app.route(\"/simple_chart\")\\ndef chart():\\n    legend = \\'Monthly Data\\'\\n    labels = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \"July\", \"August\"]\\n    values = [10, 9, 8, 7, 6, 4, 7, 8]\\n    return render_template(\\'chart.html\\', values=values, labels=labels, legend=legend)\\n\\n\\n@app.route(\"/line_chart\")\\ndef line_chart():\\n    legend = \\'Temperatures\\'\\n    temperatures = list(df[\\'Close\\'])\\n    times = list(df[\\'Date\\'])\\n    return render_template(\\'line_chart.html\\', values=temperatures, labels=times, legend=legend)\\n\\n@app.route(\"/price\")\\ndef price():\\n    global df\\n    userText = request.args.get(\\'msg\\')\\n    print(userText)\\n    op=dict(df.iloc[int(userText)])#tuple(list(df.iloc[int(userText)]))\\n    print(op)\\n    #for dicts in test_list: \\n    for keys in op: \\n        op[keys] = str(op[keys]) \\n    return op\\n\\n@app.route(\"/time_chart\")\\ndef time_chart():\\n    legend = \\'Temperatures\\'\\n    temperatures = [73.7, 73.4, 73.8, 72.8, 68.7, 65.2,\\n                    61.8, 58.7, 58.2, 58.3, 60.5, 65.7,\\n                    70.2, 71.4, 71.2, 70.9, 71.3, 71.1]\\n    times = [time(hour=11, minute=14, second=15),\\n             time(hour=11, minute=14, second=30),\\n             time(hour=11, minute=14, second=45),\\n             time(hour=11, minute=15, second=00),\\n             time(hour=11, minute=15, second=15),\\n             time(hour=11, minute=15, second=30),\\n             time(hour=11, minute=15, second=45),\\n             time(hour=11, minute=16, second=00),\\n             time(hour=11, minute=16, second=15),\\n             time(hour=11, minute=16, second=30),\\n             time(hour=11, minute=16, second=45),\\n             time(hour=11, minute=17, second=00),\\n             time(hour=11, minute=17, second=15),\\n             time(hour=11, minute=17, second=30),\\n             time(hour=11, minute=17, second=45),\\n             time(hour=11, minute=18, second=00),\\n             time(hour=11, minute=18, second=15),\\n             time(hour=11, minute=18, second=30)]\\n    return render_template(\\'time_chart.html\\', values=temperatures, labels=times, legend=legend)\\n\\n\\nif __name__ == \"__main__\":\\n    app.run(\\'0.0.0.0\\')\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from flask import Flask\n",
    "from flask import render_template,request\n",
    "from datetime import time\n",
    "import pandas as pd1\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from flask_jsonpify import jsonpify\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import time\n",
    "import pandas as pd\n",
    "from hdfs import InsecureClient\n",
    "import os\n",
    "\n",
    "client_hdfs = InsecureClient('http://localhost:50070')\n",
    "\n",
    "\n",
    "def fetchcurrentmarketprice(stock):\n",
    "    stock1=stock\n",
    "    #for ticker in ticker_list1:\n",
    "    url = 'https://in.finance.yahoo.com/quote/' + stock1\n",
    "    print(url)\n",
    "    session = requests_html.HTMLSession()\n",
    "    r = session.get(url)\n",
    "    content = BeautifulSoup(r.content, 'html')\n",
    "    print(content)\n",
    "    \n",
    "    df = pd.DataFrame(content)\n",
    "    with client_hdfs.write('/in/content.csv', encoding = 'utf-8') as writer:\n",
    "        df.to_csv(writer)\n",
    "    print(\"ok\")\n",
    "'''\n",
    "\n",
    "    try:\n",
    "        price = str(content).split('data-reactid=\"32\"')[4].split('</span>')[0].replace('>','')\n",
    "        #print(str(content).split('data-reactid=\"47\"'))\n",
    "        openprice = str(content).split('data-reactid=\"49\"')[3].split('</span>')[0].replace('>','')\n",
    "        rangeobt = str(content).split('data-reactid=\"67\"')[2].split('</span>')[0]\n",
    "        #price = str(content).split('data-reactid=\"32\"')[4].split('</span>')[0].replace('>','')\n",
    "        #price = str(content).split('data-reactid=\"32\"')[4].split('</span>')[0].replace('>','')\n",
    "        #price = str(content).split('data-reactid=\"32\"')[4].split('</span>')[0].replace('>','')\n",
    "    except IndexError as e:\n",
    "        price = 0.00\n",
    "        price = price or \"0\"\n",
    "    try:\n",
    "        price = float(price.replace(',',''))\n",
    "    except ValueError as e:\n",
    "        price = 0.00\n",
    "        time.sleep(1)\n",
    "   \n",
    "    print( price)\n",
    "    print(openprice)\n",
    "    print(rangeobt)\n",
    "        #cursor.execute(_SQL, (unidecode.unidecode(ticker[0]), price, unidecode.unidecode(ticker[1]), unidecode.unidecode(ticker[2]), unidecode.unidecode(ticker[3])))\n",
    "    return price\n",
    "\n",
    "\n",
    "\n",
    "#urltofetch='https://www.usatoday.com/story/money/2020/04/22/amazon-doing-free-deliveries-food-banks-during-coronavirus-emergency/2997254001/'\n",
    "\n",
    "#alldata=parsenews(urltofetch)\n",
    "#print(alldata)\n",
    "\n",
    "#Python program to scrape website  \n",
    "#and save quotes from website \n",
    "import requests \n",
    "from bs4 import BeautifulSoup \n",
    "import csv \n",
    "import re\n",
    "import pandas as pd\n",
    "from hdfs import InsecureClient\n",
    "import os\n",
    "\n",
    "client_hdfs = InsecureClient('http://localhost:50070')\n",
    "\n",
    "def callingnews(query):\n",
    "\n",
    "    URL = \"https://www.usatoday.com/search/?q=\"+query\n",
    "    r = requests.get(URL) \n",
    "#print(r)\n",
    "  \n",
    "    soup1 = BeautifulSoup(r.content, 'html.parser') \n",
    "#print(soup)\n",
    "    \n",
    "    df = pd.DataFrame(data = soup1)\n",
    "    #Writing Dataframe to hdfs\n",
    "    with client_hdfs.write('/in/quotes.txt', encoding = 'utf-8') as writer:\n",
    "        df.to_csv(writer)\n",
    "        \n",
    "    with client_hdfs.read('/in/quotes.txt', encoding = 'utf-8') as reader:\n",
    "        print('ok',reader)\n",
    "        soup = pd.read_csv(reader,index_col=0)\n",
    "    print(soup)\n",
    "\n",
    "    quotes=[]  # a list to store quotes \n",
    "  \n",
    "\n",
    "    table1 = soup.find_all('a', attrs = {'class':'gnt_se_a gnt_se_a__hd gnt_se_a__hi'}) \n",
    "    print(table1)\n",
    "\n",
    "#table13 = table11.get_text()\n",
    "#print(table13) \n",
    "\n",
    "    table11 = soup.find_all('div', attrs = {'class':'gnt_pr'}) \n",
    "    print(table11)\n",
    "    datalist=[]\n",
    "    linksdata=[]\n",
    "#print(table11)\n",
    "    for ik in table11:\n",
    "        datalist.append(ik.get_text())\n",
    "        print(ik.get_text())\n",
    "\n",
    "    pos=0\n",
    "    listtocheck=[]\n",
    "    for ik in table1:\n",
    "        links = re.findall(\"href=[\\\"\\'](.*?)[\\\"\\']\", str(ik))\n",
    "        linksdata.append('https://www.usatoday.com'+links[0])\n",
    "        if 'story' not in links[0]:\n",
    "            listtocheck.append(pos)\n",
    "        pos+=1\n",
    "        print(links)\n",
    "\n",
    "    print(\"list check is \",listtocheck)\n",
    "\n",
    "    for ij in range(len(listtocheck)):\n",
    "        print(ij)\n",
    "        #datalist.pop(ij)\n",
    "        #linksdata.pop(ij)\n",
    "    #print(listtocheck[ij])\n",
    "\n",
    "    print(len(datalist))\n",
    "    print(len(linksdata))\n",
    "    return datalist,linksdata\n",
    "\n",
    "\n",
    "#df\n",
    "df1=pd1.read_csv('fortune500.csv')\n",
    "df=pd.DataFrame()\n",
    "app = Flask(__name__)\n",
    "\n",
    "\n",
    "@app.route(\"/parsenews\")\n",
    "def parsenews(): \n",
    "    newsinfo = request.args.get('msg')\n",
    "    URL =newsinfo.rstrip().lstrip().strip()# \"https://www.hindustantimes.com/delhi-news/protest-at-delhi-s-jama-masjid-against-citizenship-act-4-metro-stations-closed-in-area/story-q7vKj5IUdIKMExw5eGBfxI.html\"\n",
    "    #URL =\"https://www.hindustantimes.com/delhi-news/protest-at-delhi-s-jama-masjid-against-citizenship-act-4-metro-stations-closed-in-area/story-q7vKj5IUdIKMExw5eGBfxI.html\"\n",
    "    #print repr(URL)\n",
    "    r = requests.get(URL) \n",
    "    #print(r)\n",
    "    soup = BeautifulSoup(r.content, 'html.parser') \n",
    "  \n",
    "    quotes=[]  # a list to store quotes \n",
    "  \n",
    "    table = soup.find('div', attrs = {'class':'gnt_ar_b'}) \n",
    "    #print(table)\n",
    "    alltestdata='<a href=\\''+URL+'\\' target=\"_blank\" >'+URL+'</a>'+'<br>'\n",
    "    print(alltestdata)\n",
    "    try:\n",
    "        table1 = table.find_all('p')\n",
    "        \n",
    "        for row in table.find_all('p'):\n",
    "            quote = {} \n",
    "            quote['data'] = row.text \n",
    "            alltestdata=alltestdata+row.text+\" \"\n",
    "            quotes.append(quote)\n",
    "    except:\n",
    "        alltestdata='<a href=\\''+URL+'\\' target=\"_blank\" >'+URL+'</a>'+'<br>'\n",
    "    #print(alltestdata)\n",
    "    print(alltestdata)\n",
    "    return alltestdata\n",
    "\n",
    "@app.route(\"/searchforcompany\")\n",
    "def searchforcompany():\n",
    "    global df\n",
    "    legend = 'Stock Price data'\n",
    "    company = request.args.get('company')\n",
    "    dfop=df1.loc[df1['Name'] == company]\n",
    "    op1=str(dfop['Symbol'].iloc[0])\n",
    "    print(op1)\n",
    "    df=pd1.read_csv('data//'+op1+'.csv')\n",
    "    temperatures = list(df['Close'])\n",
    "    times = list(df['Date'])\n",
    "    \n",
    "    datalist,linksdata=callingnews(company)\n",
    "    dictis={}\n",
    "    for ims in range(len(linksdata)):\n",
    "        dictis[linksdata[ims]]=linksdata[ims]\n",
    "        \n",
    "    print(\"dictionary is \",dictis)\n",
    "    urlofsite='https://www.usatoday.com'\n",
    "    io=0\n",
    "    return render_template('line_chart.html',dictdata=dictis,links=linksdata,news=datalist, values=temperatures, labels=times, legend=legend,stockname=company,symbolis=op1)\n",
    "    #return op1\n",
    "\n",
    "@app.route(\"/futurepriceprediction\")\n",
    "def futurepriceprediction():\n",
    "    companySymbol = request.args.get('msg')\n",
    "    dictis=predictpriceofdata(companySymbol)\n",
    "    #print('price is')\n",
    "    print(dictis)\n",
    "    #print(sendingcompaniesinfo)\n",
    "    return dictis\n",
    "\n",
    "    \n",
    "    \n",
    "@app.route(\"/fetchprice\")\n",
    "def fetchprice():\n",
    "    company = request.args.get('msg')\n",
    "    priceis=getpriceinfo(company)#'1211'#fetchcurrentmarketprice(company)\n",
    "    print('price is')\n",
    "    print(priceis)\n",
    "    #print(sendingcompaniesinfo)\n",
    "    return str(priceis)\n",
    "\n",
    "\n",
    "@app.route(\"/getqoutetableval\")\n",
    "def getqoutetableval():\n",
    "    company = request.args.get('msg')\n",
    "    print('company for qoute '+company)\n",
    "    qoute=getqoutetable(company)#'1211'#fetchcurrentmarketprice(company)\n",
    "    print('qoute is')\n",
    "    print(qoute)\n",
    "    df_list = qoute.values.tolist()\n",
    "    alldata=''\n",
    "    for ik in range(len(df_list)):\n",
    "        alldata=alldata+str(df_list[ik][0])+\" :- \"+str(df_list[ik][1])+\"<br>\\n\"\n",
    "    #JSONP_data = jsonpify(df_list)\n",
    "    #print(sendingcompaniesinfo)\n",
    "    return alldata\n",
    "\n",
    "@app.route(\"/\")\n",
    "def index():\n",
    "    temperatures = dict(df1['Name'])\n",
    "    sendingcompaniesinfo={}\n",
    "    for keys in temperatures: \n",
    "        temperatures[keys] = str(temperatures[keys]) \n",
    "        sendingcompaniesinfo[temperatures[keys]]='null'\n",
    "    #print(sendingcompaniesinfo)\n",
    "    return render_template('searching.html', values=sendingcompaniesinfo)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@app.route(\"/simple_chart\")\n",
    "def chart():\n",
    "    legend = 'Monthly Data'\n",
    "    labels = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \"July\", \"August\"]\n",
    "    values = [10, 9, 8, 7, 6, 4, 7, 8]\n",
    "    return render_template('chart.html', values=values, labels=labels, legend=legend)\n",
    "\n",
    "\n",
    "@app.route(\"/line_chart\")\n",
    "def line_chart():\n",
    "    legend = 'Temperatures'\n",
    "    temperatures = list(df['Close'])\n",
    "    times = list(df['Date'])\n",
    "    return render_template('line_chart.html', values=temperatures, labels=times, legend=legend)\n",
    "\n",
    "@app.route(\"/price\")\n",
    "def price():\n",
    "    global df\n",
    "    userText = request.args.get('msg')\n",
    "    print(userText)\n",
    "    op=dict(df.iloc[int(userText)])#tuple(list(df.iloc[int(userText)]))\n",
    "    print(op)\n",
    "    #for dicts in test_list: \n",
    "    for keys in op: \n",
    "        op[keys] = str(op[keys]) \n",
    "    return op\n",
    "\n",
    "@app.route(\"/time_chart\")\n",
    "def time_chart():\n",
    "    legend = 'Temperatures'\n",
    "    temperatures = [73.7, 73.4, 73.8, 72.8, 68.7, 65.2,\n",
    "                    61.8, 58.7, 58.2, 58.3, 60.5, 65.7,\n",
    "                    70.2, 71.4, 71.2, 70.9, 71.3, 71.1]\n",
    "    times = [time(hour=11, minute=14, second=15),\n",
    "             time(hour=11, minute=14, second=30),\n",
    "             time(hour=11, minute=14, second=45),\n",
    "             time(hour=11, minute=15, second=00),\n",
    "             time(hour=11, minute=15, second=15),\n",
    "             time(hour=11, minute=15, second=30),\n",
    "             time(hour=11, minute=15, second=45),\n",
    "             time(hour=11, minute=16, second=00),\n",
    "             time(hour=11, minute=16, second=15),\n",
    "             time(hour=11, minute=16, second=30),\n",
    "             time(hour=11, minute=16, second=45),\n",
    "             time(hour=11, minute=17, second=00),\n",
    "             time(hour=11, minute=17, second=15),\n",
    "             time(hour=11, minute=17, second=30),\n",
    "             time(hour=11, minute=17, second=45),\n",
    "             time(hour=11, minute=18, second=00),\n",
    "             time(hour=11, minute=18, second=15),\n",
    "             time(hour=11, minute=18, second=30)]\n",
    "    return render_template('time_chart.html', values=temperatures, labels=times, legend=legend)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run('0.0.0.0')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['1y Target Est', 303.56], ['52 Week Range', '170.27 - 327.85'], ['Ask', '290.78 x 1400'], ['Avg. Volume', 50819157.0], ['Beta (5Y Monthly)', 1.17], ['Bid', '290.20 x 1200'], [\"Day's Range\", '285.85 - 299.00'], ['EPS (TTM)', 12.73], ['Earnings Date', 'Jul 28, 2020 - Aug 03, 2020'], ['Ex-Dividend Date', 'Feb 07, 2020'], ['Forward Dividend & Yield', '3.08 (1.05%)'], ['Market Cap', '1.265T'], ['Open', 286.25], ['PE Ratio (TTM)', 22.71], ['Previous Close', 293.8], ['Quote Price', 289.07000732421875], ['Volume', 60154175.0]]\n",
      "1y Target Est\n",
      "303.56\n",
      "52 Week Range\n",
      "Ask\n",
      "1y Target Est303.56\n",
      "52 Week Range170.27 - 327.85\n",
      "Ask290.78 x 1400\n",
      "Avg. Volume50819157.0\n",
      "Beta (5Y Monthly)1.17\n",
      "Bid290.20 x 1200\n",
      "Day's Range285.85 - 299.00\n",
      "EPS (TTM)12.73\n",
      "Earnings DateJul 28, 2020 - Aug 03, 2020\n",
      "Ex-Dividend DateFeb 07, 2020\n",
      "Forward Dividend & Yield3.08 (1.05%)\n",
      "Market Cap1.265T\n",
      "Open286.25\n",
      "PE Ratio (TTM)22.71\n",
      "Previous Close293.8\n",
      "Quote Price289.07000732421875\n",
      "Volume60154175.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from flask_jsonpify import jsonpify\n",
    "qoute=getqoutetable('amzn')#'1211'#fetchcurrentmarketprice(company)\n",
    "#print('qoute is')\n",
    "#print(qoute)\n",
    "df_list = qoute.values.tolist()\n",
    "print(df_list)\n",
    "print(df_list[0][0])\n",
    "print(df_list[0][1])\n",
    "print(df_list[1][0])\n",
    "print(df_list[2][0])\n",
    "\n",
    "alldata=''\n",
    "for ik in range(len(df_list)):\n",
    "    alldata=alldata+str(df_list[ik][0])+str(df_list[ik][1])+\"\\n\"\n",
    "\n",
    "print(alldata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
